{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walk through 3D lidar data\n",
    "How does a self-driving car see the world? You can finally answer this question by walking through what a self-driving car sees. I downloaded the sample data from the AEV dataset, which you can find here: https://www.audi-electronics-venture.de/aev/web/de/driving-dataset/dataset.html. \n",
    "Download and extract the a2d2-preview folder in the same folder as this notebook :)\n",
    "\n",
    "If this is too much code you can also download samples from other datasets, for example the KITTI dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import colorsys\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "with open ('a2d2-preview/cams_lidars.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "    \n",
    "EPSILON = 1.0e-10 # norm should not be small\n",
    "def get_axes_of_a_view(view):\n",
    "    x_axis = view['x-axis']\n",
    "    y_axis = view['y-axis']\n",
    "     \n",
    "    x_axis_norm = la.norm(x_axis)\n",
    "    y_axis_norm = la.norm(y_axis)\n",
    "    \n",
    "    if (x_axis_norm < EPSILON or y_axis_norm < EPSILON):\n",
    "        raise ValueError(\"Norm of input vector(s) too small.\")\n",
    "        \n",
    "    # normalize the axes\n",
    "    x_axis = x_axis / x_axis_norm\n",
    "    y_axis = y_axis / y_axis_norm\n",
    "    \n",
    "    # make a new y-axis which lies in the original x-y plane, but is orthogonal to x-axis\n",
    "    y_axis = y_axis - x_axis * np.dot(y_axis, x_axis)\n",
    " \n",
    "    # create orthogonal z-axis\n",
    "    z_axis = np.cross(x_axis, y_axis)\n",
    "    \n",
    "    # calculate and check y-axis and z-axis norms\n",
    "    y_axis_norm = la.norm(y_axis)\n",
    "    z_axis_norm = la.norm(z_axis)\n",
    "    \n",
    "    if (y_axis_norm < EPSILON) or (z_axis_norm < EPSILON):\n",
    "        raise ValueError(\"Norm of view axis vector(s) too small.\")\n",
    "        \n",
    "    # make x/y/z-axes orthonormal\n",
    "    y_axis = y_axis / y_axis_norm\n",
    "    z_axis = z_axis / z_axis_norm\n",
    "    \n",
    "    return x_axis, y_axis, z_axis\n",
    "def get_origin_of_a_view(view):\n",
    "    return view['origin']\n",
    "\n",
    "def get_transform_from_global(view):\n",
    "    # get transform to global\n",
    "    transform_to_global = get_transform_to_global(view)\n",
    "    trans = np.eye(4)\n",
    "    rot = np.transpose(transform_to_global[0:3, 0:3])\n",
    "    trans[0:3, 0:3] = rot\n",
    "    trans[0:3, 3] = np.dot(rot, -transform_to_global[0:3, 3])\n",
    "\n",
    "    return trans\n",
    "\n",
    "def get_transform_to_global(view):\n",
    "    # get axes\n",
    "    x_axis, y_axis, z_axis = get_axes_of_a_view(view)\n",
    "    \n",
    "    # get origin \n",
    "    origin = get_origin_of_a_view(view)\n",
    "    transform_to_global = np.eye(4)\n",
    "    \n",
    "    # rotation\n",
    "    transform_to_global[0:3, 0] = x_axis\n",
    "    transform_to_global[0:3, 1] = y_axis\n",
    "    transform_to_global[0:3, 2] = z_axis\n",
    "    \n",
    "    # origin\n",
    "    transform_to_global[0:3, 3] = origin\n",
    "    \n",
    "    return transform_to_global\n",
    "\n",
    "def transform_from_to(src, target):\n",
    "    transform = np.dot(get_transform_from_global(target), \\\n",
    "                       get_transform_to_global(src))\n",
    "    \n",
    "    return transform\n",
    "def project_lidar_from_to(lidar, src_view, target_view, key_points = 'points'):\n",
    "    lidar = dict(lidar)\n",
    "    trans = transform_from_to(src_view, target_view)\n",
    "    points = lidar[key_points]\n",
    "    points_hom = np.ones((points.shape[0], 4))\n",
    "    points_hom[:, 0:3] = points\n",
    "    points_trans = (np.dot(trans, points_hom.T)).T \n",
    "    lidar[key_points] = points_trans[:,0:3]\n",
    "    \n",
    "    return lidar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "names = [(\"front_center\", \"frontcenter\"), (\"front_left\", \"frontleft\"), (\"front_right\", \"frontright\"), (\"side_left\", \"sideleft\"), (\"side_right\", \"sideright\"), (\"rear_center\", \"rearcenter\")]\n",
    "scans = list()\n",
    "for foldername, lidarname in names:\n",
    "    file_name_lidar = 'a2d2-preview/camera_lidar/20190401_145936/lidar/cam_'+foldername+'/20190401145936_lidar_'+lidarname+'_000017975.npz'\n",
    "    lidar_front_center = np.load(file_name_lidar)\n",
    "    vehicle_view = target_view = config['vehicle']['view']\n",
    "    src_view_front_center = config['cameras'][foldername]['view']\n",
    "    lidar_front_center = project_lidar_from_to(lidar_front_center,\\\n",
    "                                              src_view_front_center, \\\n",
    "                                              vehicle_view, key_points='pcloud_points')\n",
    "\n",
    "\n",
    "    tosave = np.concatenate((lidar_front_center['pcloud_points'],np.expand_dims(lidar_front_center['pcloud_attr.reflectance'], axis=1)), axis=1)\n",
    "    scan = np.array(tosave, dtype=np.float32)\n",
    "    scan[:,:3] *= 0.25\n",
    "    scan[:,3] /=255.0\n",
    "    scans.append(scan)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tosave = np.concatenate(scans, axis=0)\n",
    "tosave = tosave[:,[0,2,1,3]]\n",
    "tosave.byteswap().tofile('cars3.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walk through semantically annotated data\n",
    "In case you want to know what class a point belongs to: we can find this by projecting semantic segmentation onto the lidar points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_lidar = 'a2d2-preview/camera_lidar_semantic/20181107_132730/lidar/cam_front_center/20181107132730_lidar_frontcenter_000005599.npz'\n",
    "file_name_image = 'a2d2-preview/camera_lidar_semantic/20181107_132730/label/cam_front_center/20181107132730_label_frontcenter_000005599.png'\n",
    "lidar_front_center = np.load(file_name_lidar)\n",
    "vehicle_view = target_view = config['vehicle']['view']\n",
    "src_view_front_center = config['cameras'][foldername]['view']\n",
    "lidar_front_center = project_lidar_from_to(lidar_front_center,\\\n",
    "                                          src_view_front_center, \\\n",
    "                                          vehicle_view, key_points='points')\n",
    "\n",
    "\n",
    "tosave = np.concatenate((lidar_front_center['points'],np.expand_dims(lidar_front_center['reflectance'], axis=1)), axis=1)\n",
    "scan = np.array(tosave, dtype=np.float32)\n",
    "scan[:,:3] *= 0.25\n",
    "scan[:,3] /=255.0\n",
    "\n",
    "image = plt.imread(file_name_image)\n",
    "colors = list()\n",
    "for row, col in zip(lidar_front_center['row'], lidar_front_center['col']):\n",
    "    h, _, _ = colorsys.rgb_to_hsv(*image[int(row)][int(col)])\n",
    "    colors.append(h)\n",
    "    \n",
    "scan[:,3] = np.array(colors)\n",
    "scan = scan[:,[0,2,1,3]]\n",
    "scan.byteswap().tofile('cars4.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
